{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, re, multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cmd):    \n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    \n",
    "    if error.strip() != \"\":\n",
    "        print(error)\n",
    "\n",
    "    print(out)\n",
    "\n",
    "def __do_runNSave(cmd, out_path, ret_times, get_times_eye=True):\n",
    "     # result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    out = out.rstrip()\n",
    "    # print(\"out:\", out)\n",
    "    # print(\"error:\", error)\n",
    "        \n",
    "    with open(out_path, 'w') as fh:\n",
    "        fh.write(out)\n",
    "    \n",
    "    if \"** ERROR **\" in error:\n",
    "        print(\"ERROR:\", error)\n",
    "    \n",
    "    elif get_times_eye:\n",
    "        netw_time = int(re.search(\"networking \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        reas_time = int(re.search(\"reasoning \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        ret_times.append(netw_time); ret_times.append(reas_time)\n",
    "        # return netw_time, reas_time\n",
    "\n",
    "def runNSave_timed(cmd, out_path, max_time, get_times_eye=True):\n",
    "    mngr = multiprocess.Manager()\n",
    "    ret_times = mngr.list()\n",
    "    \n",
    "    p = multiprocess.Process(target=__do_runNSave, args=(cmd, out_path, ret_times, get_times_eye))\n",
    "    p.start()\n",
    "    \n",
    "    p.join(max_time)\n",
    "    if p.is_alive():\n",
    "        p.kill()\n",
    "        return (-1, -1)\n",
    "    else:\n",
    "        return ret_times\n",
    "        \n",
    "def runNSave(cmd, out_path, get_times_eye=True):\n",
    "   ret_times = []\n",
    "   __do_runNSave(cmd, out_path, ret_times, get_times_eye)\n",
    "   \n",
    "   return ret_times\n",
    "\n",
    "def record_eye(times_file, query, data, type, phase, netw_time, reas_time):\n",
    "    times_file.write(f\"{query},{data},{type},{phase},{netw_time},{reas_time}\\n\")\n",
    "    \n",
    "def record_sparql(times_file, query, data, load_time, exec_time):\n",
    "    times_file.write(f\"{query},{data},{load_time},{exec_time}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SPARQL PP into N3 PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for now run with Python 3.11.5 (manually fixed issue with NegatedPropertySet & reverse paths)\n",
    "\n",
    "from rdflib.plugins.sparql import parser\n",
    "from convert_pp_sparql_n3 import To_N3_Visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if someone else is running this notebook:\n",
    "\n",
    "# @rdflib.plugins.sparql.parserutils.py#279\n",
    "# add:\n",
    "# elif isinstance(t,CompValue) or isinstance(t,URIRef):\n",
    "#     res['part'] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert single query\n",
    "\n",
    "# query = \"SELECT ?x WHERE { ?x (:p1/:p2)* ?z ; !(:p3|:p4|:p5) ?a }\"\n",
    "# query = \"PREFIX : <http://example.org/gmark/> \" + \\\n",
    "#     \"SELECT * WHERE { ?x0 !(^:p1|:p2) ?x3 } \"\n",
    "#     # \"SELECT * WHERE { ?x0 ((^:p1/:p2*)?/:p3)+ ?x3 } \"\n",
    "#     # \"SELECT * WHERE { ?x0 !(:p1|^:p2|:p3) ?x3 }\"\n",
    "query = \"\"\"PREFIX : <http://example.org/gmark/> \n",
    "SELECT DISTINCT ?x2 ?x1 ?x3 ?x0 \n",
    "WHERE {  {  ?x0 (((^:p1/:p1/^:p1/:p1)))* ?x1 . ?x1 ((^:p1/:p1/^:p1)|(^:p1/:p1/^:p1)) ?x2 . ?x2 (((:p1/^:p1)))+ ?x3 . } }\n",
    "\"\"\"\n",
    "\n",
    "query = parser.parseQuery(query)\n",
    "query = query[1]\n",
    "\n",
    "print(To_N3_Visitor().convert(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query folder\n",
    "\n",
    "import os\n",
    "\n",
    "visitor = To_N3_Visitor()\n",
    "\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "files = list(os.listdir(path))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(os.path.join(path, file), 'r') as fh:\n",
    "        query = fh.read()\n",
    "        query = parser.parseQuery(query)\n",
    "        query = query[1]\n",
    "        \n",
    "        conv = visitor.convert(query)\n",
    "        conv = \"@prefix : <http://example.org/gmark/> .\\n\\n\" + conv\n",
    "        # print(conv)\n",
    "        \n",
    "        n3_file = file[0:file.index(\".\")] + \".n3\"\n",
    "        with open(os.path.join(path, \"n3\", n3_file), 'w') as fh2:\n",
    "            fh2.write(conv)\n",
    "        \n",
    "        # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SPARQL PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import rdflib\n",
    "from rdflib.plugins.sparql.results.csvresults import CSVResultSerializer\n",
    "from resCSV2N3 import convert as csv2n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdflib_sparql(query_file, data_file, result_file):\n",
    "    start = time.perf_counter()\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(data_file)\n",
    "    \n",
    "    query = open(query_file, 'r').read()\n",
    "    end = time.perf_counter()\n",
    "    load_time = (end - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    results = g.query(query)\n",
    "    \n",
    "    if \"ASK\" not in query:\n",
    "        with open(result_file, 'wb') as fh:\n",
    "            CSVResultSerializer(results).serialize(fh)\n",
    "    else:\n",
    "        result = str(bool(results)).lower()\n",
    "        with open(result_file, 'w') as fh:\n",
    "            fh.write(f\"result: {result}\")\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    exec_time = (end - start) * 1000\n",
    "    \n",
    "    return load_time, exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jena_sparql(query_file, data_file, result_file):\n",
    "    # print(data_file, query_file, result_file)\n",
    "    process = subprocess.Popen(['java', '-jar', \"../test/run/sparql.jar\", \"-n3\", data_file, \"-query\",\n",
    "                               query_file, \"-result\", result_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [b.decode('UTF-8') for b in process.communicate()]\n",
    "    if error.strip() != \"\":\n",
    "        print(error)\n",
    "    else:\n",
    "        out = str(out)\n",
    "        load_time = re.search(\"load:(.*)\", out).group(1)\n",
    "        exec_time = re.search(\"exec:(.*)\", out).group(1)\n",
    "        return (load_time, exec_time)\n",
    "\n",
    "    # g = Graph(); g.parse(data_file)\n",
    "    # g.query(open(query_file, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run single query\n",
    "\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "# path = \"test\"\n",
    "file = \"query-26.sparql\"\n",
    "name = file[:file.index(\".\")]\n",
    "query_file = os.path.join(path, file)\n",
    "data_file = os.path.join(path, \"data_100.nt\")\n",
    "result_file_csv = os.path.join(path, \"tmp.csv\")\n",
    "result_file_n3 = os.path.join(path, \"tmp.n3\")\n",
    "\n",
    "load_time, exec_time = rdflib_sparql(query_file, data_file, result_file_csv)\n",
    "print(load_time, exec_time)\n",
    "csv2n3(file=result_file_csv , ordered=False, out=result_file_n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-0.sparql\n",
      "query-1.sparql\n",
      "query-10.sparql\n",
      "query-11.sparql\n",
      "query-12.sparql\n",
      "query-13.sparql\n",
      "query-14.sparql\n",
      "query-15.sparql\n",
      "query-16.sparql\n",
      "query-17.sparql\n",
      "query-18.sparql\n",
      "query-19.sparql\n",
      "query-20.sparql\n",
      "query-21.sparql\n",
      "query-23.sparql\n",
      "query-24.sparql\n",
      "query-25.sparql\n",
      "query-26.sparql\n",
      "query-27.sparql\n",
      "query-28.sparql\n",
      "query-29.sparql\n",
      "query-30.sparql\n",
      "query-31.sparql\n",
      "query-32.sparql\n",
      "query-33.sparql\n",
      "query-34.sparql\n",
      "query-35.sparql\n",
      "query-36.sparql\n",
      "query-37.sparql\n",
      "query-38.sparql\n",
      "query-39.sparql\n",
      "query-4.sparql\n",
      "query-40.sparql\n",
      "query-41.sparql\n",
      "query-42.sparql\n",
      "query-43.sparql\n",
      "query-46.sparql\n",
      "query-47.sparql\n",
      "query-48.sparql\n",
      "query-49.sparql\n",
      "query-5.sparql\n",
      "query-6.sparql\n",
      "query-7.sparql\n",
      "query-8.sparql\n",
      "query-9.sparql\n"
     ]
    }
   ],
   "source": [
    "# run query folder\n",
    "\n",
    "data_size = str(100)\n",
    "system = 'rdflib'\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "\n",
    "dont_run = open(f\"dont_run/{system}.txt\", 'r').read().splitlines()\n",
    "times_file = open(os.path.join(path, \"results\", system, data_size, \"times.csv\"), 'w')\n",
    "times_file.write(\"query,data,load_time,exec_time\\n\")\n",
    "\n",
    "files = list(os.listdir(path))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue    \n",
    "    if file in dont_run:\n",
    "        continue\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    name = file[:file.index(\".\")]\n",
    "    query_file = os.path.join(path, file)\n",
    "    data_file = f\"data_{data_size}.nt\"\n",
    "    data_path = os.path.join(path, data_file)\n",
    "    result_path_csv = os.path.join(path, \"results\", system, data_size, f\"{name}.csv\")\n",
    "    result_path_n3 = os.path.join(path, \"results\", system, data_size, f\"{name}.n3\")\n",
    "    \n",
    "    match (system):\n",
    "        case 'jena':\n",
    "            sparql_fn = jena_sparql\n",
    "        case 'rdflib':\n",
    "            sparql_fn = rdflib_sparql\n",
    "    \n",
    "    try:\n",
    "        (load_time, exec_time) = sparql_fn(query_file, data_path, result_path_csv)\n",
    "        record_sparql(times_file, file, data_file, load_time, exec_time)\n",
    "        \n",
    "        csv2n3(file=result_path_csv , ordered=False, out=result_path_n3)\n",
    "\n",
    "        times_file.flush()\n",
    "    except:\n",
    "        print(\">> error <<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N3 PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize single file\n",
    "\n",
    "path = \"../../gmark_50_new/mix/n3\"\n",
    "file = \"query-6.n3\"\n",
    "or_file = os.path.join(path, file)\n",
    "norm_file = os.path.join(path, \"normalized\", file)\n",
    "\n",
    "# ground\n",
    "runNSave([\"eye\", \"--quiet\", or_file, \"--no-qvars\", \"--nope\", \"--pass-all\"], norm_file, get_times_eye=False)\n",
    "\n",
    "# normalize\n",
    "runNSave([\"eye\", \"--quiet\", norm_file, \"aux_list.n3\", \"--query\", \"list-predicate.n3\", \"--quantify\", \"http://www.w3.org/2000/10/swap/var#\", \"--nope\"], norm_file, get_times_eye=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize entire folder\n",
    "\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "for file in os.listdir(path):    \n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    \n",
    "    file = file[:file.index(\".\")] + \".n3\"\n",
    "    or_file = os.path.join(path, \"n3\", file)\n",
    "    norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # ground\n",
    "    runNSave([\"eye\", \"--quiet\", or_file, \"--no-qvars\", \"--nope\", \"--pass-all\"], norm_file, get_times_eye=False)\n",
    "    \n",
    "    # normalize\n",
    "    runNSave([\"eye\", \"--quiet\", norm_file, \"aux_list.n3\", \"--query\", \"list-predicate.n3\", \"--quantify\", \"http://www.w3.org/2000/10/swap/var#\", \"--nope\"], norm_file, get_times_eye=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run single rule\n",
    "\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "file = \"query-6.n3\"\n",
    "norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "data_path = os.path.join(path, \"data.n3\")\n",
    "\n",
    "res_file = os.path.join(path, \"n3\", \"results\", \"direct\", file)\n",
    "netw_time, reas_time = runNSave_timed([\"eye\", data_path, \"property-paths-direct.n3\", \"--query\", norm_file, \"--nope\"], res_file, max_time=5)\n",
    "print(netw_time, reas_time)\n",
    "\n",
    "# rules_file = os.path.join(path, \"test3_rule_creation.n3\")\n",
    "# netw_time, reas_time = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation.n3\", \"--nope\"], rules_file)\n",
    "# netw_time2, reas_time2 = runNSave([\"eye\", data_path, rules_file, \"--query\", norm_file, \"--nope\"], res_file)\n",
    "\n",
    "# data-red1: instantaneous\n",
    "# data-red2: 8s\n",
    "# data-red3: 16s\n",
    "# data-red4: > 2min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run folder\n",
    "\n",
    "path = \"../../gmark_50_new/mix\"\n",
    "\n",
    "data_size = str(100)\n",
    "\n",
    "times_file = open(os.path.join(path, \"n3\", \"results\", data_size, \"times.csv\"), 'w')\n",
    "times_file.write(\"query,data,type,phase,netw_time,reas_time\\n\")\n",
    "\n",
    "dont_run_direct = open('dont_run/n3_direct.txt', 'r').read().splitlines()\n",
    "dont_run_fwd = open('dont_run/n3_fwd.txt', 'r').read().splitlines()\n",
    "dont_run_bwd = open('dont_run/n3_bwd.txt', 'r').read().splitlines()\n",
    "\n",
    "files = list(os.listdir(os.path.join(path, \"n3\", \"normalized\")))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.startswith(\"query\") and not file.endswith(\".n3\"):\n",
    "        continue\n",
    "    \n",
    "    name = file[:file.index(\".\")]\n",
    "    norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "    data_file = f\"data_{data_size}.nt\"\n",
    "    data_path = os.path.join(path, data_file)\n",
    "    dir_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"direct\", file)\n",
    "    bwd_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"bwd\", file)\n",
    "    fwd_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"fwd\", file)\n",
    "    \n",
    "    if not os.path.exists(norm_file):\n",
    "        continue\n",
    "    \n",
    "    print(norm_file)\n",
    "    \n",
    "    max_time = 60\n",
    "    \n",
    "    try:\n",
    "        # # - direct\n",
    "        # if file not in dont_run_direct:\n",
    "        #     netw_time, reas_time = runNSave_timed([\"eye\", data_path, \"property-paths-direct.n3\", \"--query\", norm_file, \"--nope\"], dir_res_file, max_time=max_time)\n",
    "        #     record_eye(times_file, file, data_file, 'direct', 'n/a', netw_time, reas_time)\n",
    "        \n",
    "        # # - backward\n",
    "        # if file not in dont_run_bwd:\n",
    "        #     tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_bwd.n3\")\n",
    "        #     netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation-backwards.n3\", \"--nope\"], tmp_file)\n",
    "        #     record_eye(times_file, file, data_file, 'bwd', 'create', netw_time1, reas_time1)\n",
    "            \n",
    "        #     netw_time2, reas_time2 = runNSave_timed([\"eye\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], bwd_res_file, max_time=max_time)\n",
    "        #     record_eye(times_file, file, data_file, 'bwd', 'run', netw_time2, reas_time2)\n",
    "        #     record_eye(times_file, file, data_file, 'bwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "            \n",
    "        # - forward\n",
    "        if file not in dont_run_fwd:\n",
    "            tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_fwd.n3\")\n",
    "            netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation.n3\", \"--nope\"], tmp_file)\n",
    "            record_eye(times_file, file, data_file, 'fwd', 'create', netw_time1, reas_time1)\n",
    "            netw_time2, reas_time2 = runNSave_timed([\"eye\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], fwd_res_file, max_time=max_time)\n",
    "            record_eye(times_file, file, data_file, 'fwd', 'run', netw_time2, reas_time2)\n",
    "            record_eye(times_file, file, data_file, 'fwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "    \n",
    "    except:\n",
    "        print(\">>error<<\")\n",
    "    \n",
    "    times_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SparqLog PP on Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test\"\n",
    "in_path = os.path.join(path, \"misc/test3.sparql\")\n",
    "out_path = os.path.join(path, \"misc/test3_sparqlog.nmo\")\n",
    "\n",
    "run([\"java\", \"-jar\", \"sparql2sparqlog.jar\", in_path, out_path]) # translate\n",
    "run([\"/opt/nemo-0.6.0/nmo\", out_path, \"--export-dir\", \"results\", \"--overwrite-results\"]) # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N3 PP on Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test\"\n",
    "# substitute for rule file without collections\n",
    "in_path = os.path.join(path, \"data.n3\") \n",
    "out_path = os.path.join(path, \"data.nmo\")\n",
    "\n",
    "run([\"java\", \"-jar\", \"n32nmo.jar\", in_path, out_path]) # translate\n",
    "# run([\"/opt/nemo-0.6.0/nmo\", out_path, \"--export-dir\", \"results\", \"--overwrite-results\"]) # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "\n",
    "def msg(test, msg, to):\n",
    "    str = f\"- {test}: {msg}\"\n",
    "    log(str, to)\n",
    "    \n",
    "def log(str, to):\n",
    "    to.write(str + \"\\n\")\n",
    "    print(str)\n",
    "    to.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(file1, file2, test, to):\n",
    "    process = subprocess.Popen(['java', '-jar', \"compare_res.jar\", file1, file2], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    out = out.strip()\n",
    "    \n",
    "    if out != \"results are the same!\":\n",
    "        msg(test, out, to=to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = str(50)\n",
    "system = 'rdflib'\n",
    "\n",
    "path1 = f\"../../gmark_50_new/mix/results/{system}/{data_size}\"\n",
    "\n",
    "files = list(os.listdir(path1))\n",
    "files.sort()\n",
    "\n",
    "# for version in ('direct', 'fwd', 'bwd'):\n",
    "for version in ('fwd',):\n",
    "    with open(f\"cmp/{version}_{data_size}_out.txt\", 'w') as logfile:\n",
    "        log(f\"> comparing '{version}'\", to=logfile)\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.endswith(\".n3\"):\n",
    "                continue\n",
    "            \n",
    "            print(file)\n",
    "            file1 = os.path.join(path1, file)\n",
    "            \n",
    "            path2 = f\"../../gmark_50_new/mix/n3/results/{data_size}/{version}\"\n",
    "            file2 = os.path.join(path2, file)\n",
    "            if not os.path.isfile(file2):\n",
    "                msg(file, \"cannot find file2\", to=logfile)\n",
    "            else:\n",
    "                compare_results(file1, file2, test=file, to=logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
