{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- results/gmark/rdflib: different results (slower) compared to June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, re, multiprocess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cmd):    \n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    \n",
    "    if error.strip() != \"\":\n",
    "        print(error)\n",
    "\n",
    "    print(out)\n",
    "\n",
    "def __do_runNSave(cmd, out_path, ret_times, get_times_eye=True):\n",
    "     # result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    out = out.rstrip()\n",
    "    # print(\"out:\", out)\n",
    "    # print(\"error:\", error)\n",
    "        \n",
    "    with open(out_path, 'w') as fh:\n",
    "        fh.write(out)\n",
    "    \n",
    "    if \"** ERROR **\" in error:\n",
    "        print(\"ERROR:\", error)\n",
    "    \n",
    "    elif get_times_eye:\n",
    "        netw_time = int(re.search(\"networking \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        reas_time = int(re.search(\"reasoning \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        ret_times.append(netw_time); ret_times.append(reas_time)\n",
    "        # return netw_time, reas_time\n",
    "\n",
    "def runNSave_timed(cmd, out_path, max_time, get_times_eye=True):\n",
    "    mngr = multiprocess.Manager()\n",
    "    ret_times = mngr.list()\n",
    "    \n",
    "    p = multiprocess.Process(target=__do_runNSave, args=(cmd, out_path, ret_times, get_times_eye))\n",
    "    p.start()\n",
    "    \n",
    "    p.join(max_time)\n",
    "    if p.is_alive():\n",
    "        p.kill()\n",
    "        return (-1, -1)\n",
    "    else:\n",
    "        return ret_times\n",
    "        \n",
    "def runNSave(cmd, out_path, get_times_eye=True):\n",
    "   ret_times = []\n",
    "   __do_runNSave(cmd, out_path, ret_times, get_times_eye)\n",
    "   \n",
    "   return ret_times\n",
    "\n",
    "def record_eye(times_file, run, query, data, type, phase, netw_time, reas_time):\n",
    "    times_file.write(f\"{run},{query},{data},{type},{phase},{netw_time},{reas_time}\\n\")\n",
    "    \n",
    "def record_sparql(times_file, run, query, data, load_time, exec_time):\n",
    "    times_file.write(f\"{run},{query},{data},{load_time},{exec_time}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert N3 to NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to NT, replace first/rest pairs\n",
    "\n",
    "folder = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/SPIN-to-N3/property-paths/zika/data/new\"\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if not file.endswith(\".n3\"):\n",
    "        continue\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    name = file[:file.index(\".\")]\n",
    "    nt_file = f\"{name}.nt\"\n",
    "    \n",
    "    # convert to NT\n",
    "    \n",
    "    path = os.path.join(folder, file)\n",
    "    args = ['java', '-jar', \"../test/run/turtle2nt.jar\", \"-turtle\", path]\n",
    "    process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [b.decode('UTF-8') for b in process.communicate()]\n",
    "    \n",
    "    # rename first/rest to avoid eye builtins\n",
    "    # NOTE: rules should also use f1rst/r3st!\n",
    "    \n",
    "    out = out.replace(\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#first>\", \"<http://www.w3.org/1999/02/22-rdf-syntax-ns#f1rst>\")\n",
    "    out = out.replace(\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#rest>\", \"<http://www.w3.org/1999/02/22-rdf-syntax-ns#r3st>\")\n",
    "    \n",
    "    with open(os.path.join(folder, nt_file), 'w') as nt_fh:\n",
    "        nt_fh.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"freeze\" bnodes\n",
    "\n",
    "folder = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/SPIN-to-N3/property-paths/zika/data/new\"\n",
    "for file in os.listdir(folder):\n",
    "    if not file.endswith(\".nt\"):\n",
    "        continue\n",
    "    \n",
    "    print(file)\n",
    "    with open(os.path.join(folder, file), 'r') as nt_fh:\n",
    "        contents = nt_fh.read()\n",
    "    \n",
    "    matches = [ (m.start(0), m.end(0)) for m in re.finditer(\"(_:(.*?)) \", contents) ]    \n",
    "    array = []; prior = 0\n",
    "    for (start, end) in matches:\n",
    "        if start > 0:\n",
    "            array.extend(contents[prior : start])\n",
    "        \n",
    "        id = contents[start+2 : end-1]\n",
    "        repl = f\"<http://example.org/{id}>\"\n",
    "        array.extend(repl)\n",
    "        \n",
    "        prior = end-1\n",
    "        \n",
    "    array.extend(contents[prior : len(contents)])\n",
    "    \n",
    "    contents = \"\".join(array)\n",
    "    \n",
    "    with open(os.path.join(folder, file), 'w') as nt_fh:\n",
    "        nt_fh.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SPARQL PP into N3 PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for now run with Python 3.11.5 (manually fixed issue with NegatedPropertySet & reverse paths)\n",
    "\n",
    "from rdflib.plugins.sparql import parser\n",
    "from convert_pp_sparql_n3 import To_N3_Visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if someone else is running this notebook:\n",
    "\n",
    "# @rdflib.plugins.sparql.parserutils.py#279\n",
    "# add:\n",
    "# elif isinstance(t,CompValue) or isinstance(t,URIRef):\n",
    "#     res['part'] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert single query\n",
    "\n",
    "# query = \"SELECT ?x WHERE { ?x (:p1/:p2)* ?z ; !(:p3|:p4|:p5) ?a }\"\n",
    "# query = \"PREFIX : <http://example.org/gmark/> \" + \\\n",
    "#     \"SELECT * WHERE { ?x0 !(^:p1|:p2) ?x3 } \"\n",
    "#     # \"SELECT * WHERE { ?x0 ((^:p1/:p2*)?/:p3)+ ?x3 } \"\n",
    "#     # \"SELECT * WHERE { ?x0 !(:p1|^:p2|:p3) ?x3 }\"\n",
    "query = \"\"\"PREFIX : <http://example.org/gmark/> \n",
    "SELECT DISTINCT ?x2 ?x1 ?x3 ?x0 \n",
    "WHERE {  {  ?x0 (((^:p1/:p1/^:p1/:p1)))* ?x1 . ?x1 ((^:p1/:p1/^:p1)|(^:p1/:p1/^:p1)) ?x2 . ?x2 (((:p1/^:p1)))+ ?x3 . } }\n",
    "\"\"\"\n",
    "\n",
    "query = parser.parseQuery(query)\n",
    "query = query[1]\n",
    "\n",
    "print(To_N3_Visitor().convert(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query folder\n",
    "\n",
    "import os\n",
    "\n",
    "visitor = To_N3_Visitor()\n",
    "\n",
    "# path = \"../../gmark_50_new/mix\"\n",
    "path = \"zika\"\n",
    "files = list(os.listdir(path))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(os.path.join(path, file), 'r') as fh:\n",
    "        query = fh.read()\n",
    "        query = parser.parseQuery(query)\n",
    "        query = query[1]\n",
    "        \n",
    "        conv = visitor.convert(query)\n",
    "        # conv = \"@prefix : <http://example.org/gmark/> .\\n\\n\" + conv\n",
    "        conv = \"@prefix fh: <http://hl7.org/fhir/> .\\n\\n\" + conv\n",
    "        # print(conv)\n",
    "        \n",
    "        n3_file = file[0:file.index(\".\")] + \".n3\"\n",
    "        with open(os.path.join(path, \"n3\", n3_file), 'w') as fh2:\n",
    "            fh2.write(conv)\n",
    "        \n",
    "        # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SPARQL PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import rdflib\n",
    "from rdflib.plugins.sparql.results.csvresults import CSVResultSerializer\n",
    "from resCSV2N3 import convert as csv2n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = subprocess.Popen([\"/Users/wvw/git/n3/eclipse-rdf4j-5.1.3/bin/console.sh\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "# out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "\n",
    "# for i in range(6):\n",
    "#     print(process.stdout.readline())   \n",
    "\n",
    "# print(process.communicate(input=\"create repo\"))\n",
    "# print(process.stdout.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdflib_sparql(query_file, data_file, result_file):\n",
    "    start = time.perf_counter()\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(data_file)\n",
    "    \n",
    "    query = open(query_file, 'r').read()\n",
    "    end = time.perf_counter()\n",
    "    load_time = (end - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    results = g.query(query)\n",
    "    \n",
    "    if \"ASK\" not in query:\n",
    "        with open(result_file, 'wb') as fh:\n",
    "            CSVResultSerializer(results).serialize(fh)\n",
    "    else:\n",
    "        result = str(bool(results)).lower()\n",
    "        with open(result_file, 'w') as fh:\n",
    "            fh.write(f\"result: {result}\")\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    exec_time = (end - start) * 1000\n",
    "    \n",
    "    return load_time, exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def java_sparql(exec_path, query_file, data_file, result_file, qtype=\"select\"):\n",
    "    # print(data_file, query_file, result_file)\n",
    "    \n",
    "    args = ['java', '-jar', exec_path, \"-n3\", data_file, \"-query\", query_file, \"-result\", result_file]\n",
    "    if exec_path == \"../test/run/rdf4j_sparql.jar\":\n",
    "        args.extend([\"-type\", qtype])\n",
    "        \n",
    "    process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [b.decode('UTF-8') for b in process.communicate()]\n",
    "    # if error.strip() != \"\":\n",
    "    #     print(error)\n",
    "    # else:\n",
    "    out = str(out)\n",
    "    # print(error)\n",
    "    # print(out)\n",
    "    load_time = re.search(\"load:(.*)\", out).group(1)\n",
    "    exec_time = re.search(\"exec:(.*)\", out).group(1)\n",
    "    return (load_time, exec_time)\n",
    "\n",
    "def jena_sparql(query_file, data_file, result_file):\n",
    "    return java_sparql(\"../test/run/jena_sparql.jar\", query_file, data_file, result_file)\n",
    "\n",
    "def rdf4j_sparql(query_file, data_file, result_file):\n",
    "    qtype = \"select\"\n",
    "    with open(query_file, 'r') as query_fh:\n",
    "        if \"ASK {\" in query_fh.read():\n",
    "            qtype = \"ask\"\n",
    "    \n",
    "    return java_sparql(\"../test/run/rdf4j_sparql.jar\", query_file, data_file, result_file, qtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run query folder\n",
    "\n",
    "# path = \"../../gmark_50_new/mix\"\n",
    "# data_size = str(100)\n",
    "# skip = open(f\"skip/{data_size}/{system}.txt\", 'r').read().splitlines()\n",
    "\n",
    "path = \"zika\"\n",
    "data_size = \"7_000_pt2\"\n",
    "skip = []\n",
    "\n",
    "system = 'jena'\n",
    "nr_runs = 1\n",
    "\n",
    "times_path = os.path.join(path, \"results\", system, data_size, f\"times_{system}.csv\")\n",
    "exists = os.path.exists(times_path)\n",
    "times_file = open(times_path, 'a')\n",
    "if not exists:\n",
    "    times_file.write(\"run,query,data,load_time,exec_time\\n\")\n",
    "else:\n",
    "    times_file.write(\"\\n\")\n",
    "\n",
    "for run in range(0, nr_runs):\n",
    "    print(\"run\", \":\", run)\n",
    "    files = list(os.listdir(path))\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if not file.endswith(\".sparql\"):\n",
    "            continue    \n",
    "        if file in skip:\n",
    "            continue\n",
    "        \n",
    "        print(file, \":\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "        name = file[:file.index(\".\")]\n",
    "        query_file = os.path.join(path, file)\n",
    "        data_file = f\"data_{data_size}.nt\"\n",
    "        data_path = os.path.join(path, \"data\", data_file)\n",
    "        result_path_csv = os.path.join(path, \"results\", system, data_size, f\"{name}.csv\")\n",
    "        result_path_n3 = os.path.join(path, \"results\", system, data_size, f\"{name}.n3\")\n",
    "        \n",
    "        match (system):\n",
    "            case 'jena':\n",
    "                sparql_fn = jena_sparql\n",
    "            case 'rdflib':\n",
    "                sparql_fn = rdflib_sparql\n",
    "            case 'rdf4j':\n",
    "                sparql_fn = rdf4j_sparql\n",
    "        \n",
    "        try:\n",
    "            (load_time, exec_time) = sparql_fn(query_file, data_path, result_path_csv)\n",
    "            record_sparql(times_file, run, file, data_file, load_time, exec_time)\n",
    "            \n",
    "            \n",
    "            csv2n3(file=result_path_csv , ordered=False, out=result_path_n3)\n",
    "            times_file.flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\">> error <<\", e)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N3 PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp3.n3\n",
      "pp4-re0.n3\n",
      "pp2.n3\n",
      "pp4-re1.n3\n",
      "pp2-re0.n3\n",
      "pp2-re1.n3\n",
      "pp1.n3\n",
      "pp4.n3\n",
      "pp4-2.n3\n"
     ]
    }
   ],
   "source": [
    "# normalize entire folder\n",
    "\n",
    "# path = \"../../gmark_50_new/mix/n3\"\n",
    "path = \"zika/n3\"\n",
    "for file in os.listdir(path):    \n",
    "    if not file.endswith(\".n3\"):\n",
    "        continue\n",
    "    \n",
    "    # if \"pp4-re1\" not in file:\n",
    "    #     continue\n",
    "    \n",
    "    file = file[:file.index(\".\")] + \".n3\"\n",
    "    or_file = os.path.join(path, file)\n",
    "    norm_file = os.path.join(path, \"normalized\", file)\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # ground\n",
    "    runNSave([\"eye\", \"--quiet\", or_file, \"--no-qvars\", \"--nope\", \"--pass-all\"], norm_file, get_times_eye=False)\n",
    "    \n",
    "    # normalize\n",
    "    runNSave([\"eye\", \"--quiet\", norm_file, \"aux_list.n3\", \"--query\", \"list-predicate.n3\", \"--quantify\", \"http://www.w3.org/2000/10/swap/var#\", \"--nope\"], norm_file, get_times_eye=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup 1_000_pt2\n",
      "run: 0\n",
      "pp1.n3 : 12:03:15\n",
      "pp2-2.n3 : 12:03:15\n",
      "pp2-re0.n3 : 12:03:16\n",
      "pp2-re1.n3 : 12:03:16\n",
      "pp2.n3 : 12:03:17\n",
      "pp3.n3 : 12:03:18\n",
      "pp4-2.n3 : 12:03:19\n",
      "pp4-re0.n3 : 12:03:20\n",
      "pp4-re1.n3 : 12:03:20\n",
      "pp4.n3 : 12:03:21\n",
      "\n",
      "setup 2_000_pt2\n",
      "run: 0\n",
      "pp1.n3 : 12:03:22\n",
      "pp2-2.n3 : 12:03:22\n",
      "pp2-re0.n3 : 12:03:23\n",
      "pp2-re1.n3 : 12:03:24\n",
      "pp2.n3 : 12:03:25\n",
      "pp3.n3 : 12:03:27\n",
      "pp4-2.n3 : 12:03:29\n",
      "pp4-re0.n3 : 12:03:31\n",
      "pp4-re1.n3 : 12:03:32\n",
      "pp4.n3 : 12:03:33\n",
      "\n",
      "setup 5_000_pt2\n",
      "run: 0\n",
      "pp1.n3 : 12:03:36\n",
      "pp2-2.n3 : 12:03:38\n",
      "pp2-re0.n3 : 12:03:40\n",
      "pp2-re1.n3 : 12:03:42\n",
      "pp2.n3 : 12:03:44\n",
      "pp3.n3 : 12:03:55\n",
      "pp4-2.n3 : 12:04:01\n",
      "pp4-re0.n3 : 12:04:19\n",
      "pp4-re1.n3 : 12:04:21\n",
      "pp4.n3 : 12:04:26\n",
      "\n",
      "setup 7_000_pt2\n",
      "run: 0\n",
      "pp1.n3 : 12:04:44\n",
      "pp2-2.n3 : 12:04:47\n",
      "pp2-re0.n3 : 12:04:49\n",
      "pp2-re1.n3 : 12:04:52\n",
      "pp2.n3 : 12:04:56\n",
      "pp3.n3 : 12:05:17\n",
      "pp4-2.n3 : 12:05:28\n",
      "pp4-re0.n3 : 12:06:05\n",
      "pp4-re1.n3 : 12:06:08\n",
      "pp4.n3 : 12:06:18\n",
      "\n",
      "setup 10_000_pt2\n",
      "run: 0\n",
      "pp1.n3 : 12:06:55\n",
      "pp2-2.n3 : 12:07:00\n",
      "pp2-re0.n3 : 12:07:03\n",
      "pp2-re1.n3 : 12:07:07\n",
      "pp2.n3 : 12:07:15\n",
      "pp3.n3 : 12:08:10\n",
      "pp4-2.n3 : 12:08:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wvw/anaconda3/lib/python3.11/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/wvw/anaconda3/lib/python3.11/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/b6/7128wh613rqcbppftg_0hb2h0000gn/T/ipykernel_45451/3222760756.py\", line 13, in __do_runNSave\n",
      "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wvw/anaconda3/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wvw/anaconda3/lib/python3.11/subprocess.py\", line 2108, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wvw/anaconda3/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>error<<\n",
      "pp4-re0.n3 : 12:41:12\n",
      "pp4-re1.n3 : 12:41:16\n"
     ]
    }
   ],
   "source": [
    "# run folder\n",
    "\n",
    "# path = \"../../gmark_50_new/mix\"\n",
    "# data_size = str(100)\n",
    "# skip_direct = open(f'skip/{data_size}/n3_direct.txt', 'r').read().splitlines()\n",
    "# skip_fwd = open(f'skip/{data_size}/n3_fwd.txt', 'r').read().splitlines()\n",
    "# skip_bwd = open(f'skip/{data_size}/n3_bwd.txt', 'r').read().splitlines()\n",
    "\n",
    "# for only_query in [ \"pp4-re1.n3\" ]:\n",
    "for data_size in [ \"1_000_pt2\", \"2_000_pt2\", \"5_000_pt2\", \"7_000_pt2\", \"10_000_pt2\" ]:\n",
    "    # print(\"setup\", only_query, data_size)\n",
    "    print(\"setup\", data_size)\n",
    "    \n",
    "    path = \"zika\"\n",
    "    # data_size = \"10_000_pt2\"\n",
    "    skip_direct = []\n",
    "    skip_bwd = []\n",
    "    skip_fwd = []\n",
    "    skip_mxd = []\n",
    "\n",
    "    nr_runs = 1\n",
    "\n",
    "    times_path = os.path.join(path, \"n3\", \"results\", data_size, \"times_n3.csv\")\n",
    "    exists = os.path.exists(times_path)\n",
    "    times_file = open(times_path, 'a')\n",
    "    if not exists:\n",
    "        times_file.write(\"run,query,data,type,phase,netw_time,reas_time\\n\")\n",
    "    else:\n",
    "        times_file.write(\"\\n\")\n",
    "\n",
    "    for run in range(0, nr_runs):\n",
    "        print(\"run:\", run)\n",
    "        \n",
    "        files = list(os.listdir(os.path.join(path, \"n3\", \"normalized\")))\n",
    "        files.sort()\n",
    "        for file in files:        \n",
    "            if not file.endswith(\".n3\"):\n",
    "                continue\n",
    "            \n",
    "            # if only_query not in file:\n",
    "            #     continue\n",
    "            \n",
    "            name = file[:file.index(\".\")]\n",
    "            norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "            data_file = f\"data_{data_size}.nt\"\n",
    "            data_path = os.path.join(path, \"data\", data_file)\n",
    "            dir_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"direct\", file)\n",
    "            bwd_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"bwd\", file)\n",
    "            fwd_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"fwd\", file)\n",
    "            mxd_res_file = os.path.join(path, \"n3\", \"results\", data_size, \"mxd\", file)\n",
    "            \n",
    "            if not os.path.exists(norm_file):\n",
    "                continue\n",
    "            \n",
    "            print(file, \":\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            \n",
    "            max_time = 300\n",
    "            \n",
    "            try:\n",
    "                # # - direct\n",
    "                # if file not in skip_direct:\n",
    "                #     netw_time, reas_time = runNSave_timed([\"eye\", \"--turtle\", data_path, \"property-paths-direct.n3\", \"--query\", norm_file, \"--nope\"], dir_res_file, max_time=max_time)\n",
    "                #     record_eye(times_file, run, file, data_file, 'direct', 'n/a', netw_time, reas_time)\n",
    "                \n",
    "                # # - backward\n",
    "                # if file not in skip_bwd:\n",
    "                #     tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_bwd.n3\")\n",
    "                #     netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation-backwards.n3\", \"--nope\"], tmp_file)\n",
    "                #     record_eye(times_file, run, file, data_file, 'bwd', 'create', netw_time1, reas_time1)\n",
    "                #     netw_time2, reas_time2 = runNSave_timed([\"eye\", \"--turtle\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], bwd_res_file, max_time=max_time)\n",
    "                #     record_eye(times_file, run, file, data_file, 'bwd', 'run', netw_time2, reas_time2)\n",
    "                #     record_eye(times_file, run, file, data_file, 'bwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "                \n",
    "                # - backward (auto re-ordering)\n",
    "                if file not in skip_bwd:\n",
    "                    tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_bwd_auto.n3\")\n",
    "                    netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"scores.n3\", \"--query\", \"rule-creation-bwd_opt.n3\", \"--nope\", \n",
    "                                                        \"--quantify\", \"http://www.w3.org/2000/10/swap/var\"], tmp_file)\n",
    "                    record_eye(times_file, run, file, data_file, 'bwd_auto', 'create', netw_time1, reas_time1)\n",
    "                    netw_time2, reas_time2 = runNSave_timed([\"eye\", \"--turtle\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], bwd_res_file, max_time=max_time)\n",
    "                    record_eye(times_file, run, file, data_file, 'bwd_auto', 'run', netw_time2, reas_time2)\n",
    "                    record_eye(times_file, run, file, data_file, 'bwd_auto', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "                \n",
    "                # # - backward (manual re-ordering)\n",
    "                # for id in [ \"ro1\" ]:\n",
    "                #     tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_bwd_{id}.n3\")\n",
    "                #     netw_time1, reas_time1 = (0, 0)\n",
    "                #     netw_time2, reas_time2 = runNSave_timed([\"eye\", \"--turtle\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], bwd_res_file, max_time=max_time)\n",
    "                #     record_eye(times_file, run, file, data_file, f'bwd_{id}', 'run', netw_time2, reas_time2)\n",
    "                #     record_eye(times_file, run, file, data_file, f'bwd_{id}', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "                    \n",
    "                # # - forward\n",
    "                # if file not in skip_fwd:\n",
    "                #     tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_fwd.n3\")\n",
    "                #     netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation.n3\", \"--nope\"], tmp_file)\n",
    "                #     record_eye(times_file, run, file, data_file, 'fwd', 'create', netw_time1, reas_time1)\n",
    "                #     netw_time2, reas_time2 = runNSave_timed([\"eye\", \"--turtle\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], fwd_res_file, max_time=max_time)\n",
    "                #     record_eye(times_file, run, file, data_file, 'fwd', 'run', netw_time2, reas_time2)\n",
    "                #     record_eye(times_file, run, file, data_file, 'fwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "            \n",
    "                # # - mixed\n",
    "                # if file not in skip_mxd:\n",
    "                #     tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_mxd.n3\")\n",
    "                #     netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation_mixed.n3\", \"--nope\"], tmp_file)\n",
    "                #     record_eye(times_file, run, file, data_file, 'mxd', 'create', netw_time1, reas_time1)\n",
    "                #     netw_time2, reas_time2 = runNSave_timed([\"eye\", \"--turtle\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], mxd_res_file, max_time=max_time)\n",
    "                #     record_eye(times_file, run, file, data_file, 'mxd', 'run', netw_time2, reas_time2)\n",
    "                #     record_eye(times_file, run, file, data_file, 'mxd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "            \n",
    "            except:\n",
    "                print(\">>error<<\")\n",
    "            \n",
    "            times_file.flush()\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SparqLog PP on Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test\"\n",
    "in_path = os.path.join(path, \"misc/test3.sparql\")\n",
    "out_path = os.path.join(path, \"misc/test3_sparqlog.nmo\")\n",
    "\n",
    "run([\"java\", \"-jar\", \"sparql2sparqlog.jar\", in_path, out_path]) # translate\n",
    "run([\"/opt/nemo-0.6.0/nmo\", out_path, \"--export-dir\", \"results\", \"--overwrite-results\"]) # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N3 PP on Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test\"\n",
    "# substitute for rule file without collections\n",
    "in_path = os.path.join(path, \"data.n3\") \n",
    "out_path = os.path.join(path, \"data.nmo\")\n",
    "\n",
    "run([\"java\", \"-jar\", \"n32nmo.jar\", in_path, out_path]) # translate\n",
    "# run([\"/opt/nemo-0.6.0/nmo\", out_path, \"--export-dir\", \"results\", \"--overwrite-results\"]) # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "\n",
    "def msg(test, msg, to):\n",
    "    str = f\"- {test}: {msg}\"\n",
    "    log(str, to)\n",
    "    \n",
    "def log(str, to):\n",
    "    to.write(str + \"\\n\")\n",
    "    print(str)\n",
    "    to.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(file1, file2, test, to):\n",
    "    process = subprocess.Popen(['java', '-jar', \"compare_res.jar\", file1, file2], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    out = out.strip()\n",
    "    \n",
    "    # print(error)\n",
    "    if out != \"results are the same!\":\n",
    "        msg(test, out, to=to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_size = str(100)\n",
    "# system = 'rdf4j'\n",
    "# path = \"../../gmark_50_new/mix\"\n",
    "\n",
    "data_size = \"7_000_pt2\"\n",
    "system = 'jena'\n",
    "path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/SPIN-to-N3/property-paths/zika\"\n",
    "\n",
    "files = list(os.listdir(os.path.join(path, \"results\", system, data_size)))\n",
    "files.sort()\n",
    "for version in ('direct', 'fwd', 'bwd'):\n",
    "# for version in ('fwd',):\n",
    "    with open(f\"cmp/{version}_{data_size}_out.txt\", 'w') as logfile:\n",
    "        log(f\"> comparing '{version}'\", to=logfile)\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.endswith(\".n3\"):\n",
    "                continue\n",
    "            \n",
    "            print(file)\n",
    "            \n",
    "            file1 = os.path.join(path, \"results\", system, data_size, file)\n",
    "            file2 = os.path.join(path, \"n3\", \"results\", data_size, version, file)\n",
    "            \n",
    "            if not os.path.isfile(file2):\n",
    "                msg(file, \"cannot find file2\", to=logfile)\n",
    "            else:\n",
    "                # too slow for files sized 2MB\n",
    "                # compare_results(file1, file2, test=file, to=logfile)\n",
    "                \n",
    "                with open(file1, \"rb\") as f1:\n",
    "                    num1 = sum(1 for _ in f1)\n",
    "                with open(file2, \"rb\") as f2:\n",
    "                    num2 = sum(1 for _ in f2)\n",
    "                    num2 = (0 if num2 <= 5 else num2 - 7) # (comments at top & bottom of file)\n",
    "                if num1 != num2:\n",
    "                    msg(file, f\"different # results: {num1} (sparql) <> {num2} (n3)\", to=logfile)\n",
    "                # else:\n",
    "                #     print(f\"same # results: {num1} (sparql) == {num2} (n3)\")\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
