{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNSave(cmd, path, get_times=True):\n",
    "    # result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    out = out.rstrip()\n",
    "    # print(\"out:\", out)\n",
    "    # print(\"error:\", error)\n",
    "        \n",
    "    with open(path, 'w') as fh:\n",
    "        fh.write(out)\n",
    "    \n",
    "    if \"** ERROR **\" in error:\n",
    "        print(\"ERROR:\", error)\n",
    "    \n",
    "    elif get_times:\n",
    "        netw_time = int(re.search(\"networking \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        reas_time = int(re.search(\"reasoning \\d+ \\[msec cputime\\] (\\d+) \\[msec walltime\\]\", error).group(1))\n",
    "        return netw_time, reas_time\n",
    "\n",
    "def record(times_file, query, data, type, phase, netw_time, reas_time):\n",
    "    times_file.write(f\"{query},{data},{type},{phase},{netw_time},{reas_time}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SPARQL PP into N3 PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for now run with Python 3.11.5 (manually fixed issue with NegatedPropertySet & reverse paths)\n",
    "\n",
    "from rdflib.plugins.sparql import parser\n",
    "from convert_pp_sparql_n3 import To_N3_Visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdflib.plugins.sparql.parserutils.py#279\n",
    "# add:\n",
    "# elif isinstance(t,CompValue) or isinstance(t,URIRef):\n",
    "#     res['part'] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert single query\n",
    "\n",
    "query = \"SELECT ?x WHERE { ?x (:p1/:p2)* ?z ; !(:p3|:p4|:p5) ?a }\"\n",
    "query = \"PREFIX : <http://example.org/gmark/> \" + \\\n",
    "    \"SELECT * WHERE { ?x0 !(^:p1|:p2) ?x3 } \"\n",
    "    # \"SELECT * WHERE { ?x0 ((^:p1/:p2*)?/:p3)+ ?x3 } \"\n",
    "    # \"SELECT * WHERE { ?x0 !(:p1|^:p2|:p3) ?x3 }\"\n",
    "\n",
    "query = parser.parseQuery(query)\n",
    "query = query[1]\n",
    "\n",
    "print(To_N3_Visitor().convert(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query folder\n",
    "\n",
    "import os\n",
    "\n",
    "visitor = To_N3_Visitor()\n",
    "\n",
    "path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix\"\n",
    "files = list(os.listdir(path))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(os.path.join(path, file), 'r') as fh:\n",
    "        query = fh.read()\n",
    "        query = parser.parseQuery(query)\n",
    "        query = query[1]\n",
    "        \n",
    "        conv = visitor.convert(query)\n",
    "        conv = \"@prefix : <http://example.org/gmark/> .\\n\\n\" + conv\n",
    "        # print(conv)\n",
    "        \n",
    "        n3_file = file[0:file.index(\".\")] + \".n3\"\n",
    "        with open(os.path.join(path, \"n3\", n3_file), 'w') as fh2:\n",
    "            fh2.write(conv)\n",
    "            \n",
    "        # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SPARQL PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resCSV2N3 import convert as csv2n3\n",
    "# from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_sparql(query_file, data_file, result_file):\n",
    "    process = subprocess.Popen(['java', '-jar', \"../test/run/sparql.jar\", \"-n3\", data_file, \"-query\", query_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    # print(out); print(error)\n",
    "    \n",
    "    # g = Graph(); g.parse(data_file)\n",
    "    # g.query(open(query_file, 'r').read())\n",
    "    \n",
    "    with open(result_file, 'w') as fh:\n",
    "        fh.write(out.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix\"\n",
    "files = list(os.listdir(path))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    # print(file)\n",
    "    \n",
    "    name = file[:file.index(\".\")]\n",
    "    query_file = os.path.join(path, file)\n",
    "    data_file = os.path.join(path, \"data.n3\")\n",
    "    result_file_csv = os.path.join(path, \"results\", f\"{name}.csv\")\n",
    "    result_file_n3 = os.path.join(path, \"results\", f\"{name}.n3\")\n",
    "    \n",
    "    exec_sparql(query_file, data_file, result_file_csv)\n",
    "    \n",
    "    csv2n3(file=result_file_csv , ordered=False, out=result_file_n3)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N3 PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize single file\n",
    "\n",
    "path = \"test\"\n",
    "or_file = os.path.join(path, \"test2.n3\")\n",
    "norm_file = os.path.join(path, \"test2_norm.n3\")\n",
    "\n",
    "# ground\n",
    "runNSave([\"eye\", \"--quiet\", or_file, \"--no-qvars\", \"--nope\", \"--pass-all\"], norm_file, get_times=False)\n",
    "    \n",
    "# normalize\n",
    "runNSave([\"eye\", \"--quiet\", norm_file, \"aux_list.n3\", \"--query\", \"list-predicate.n3\", \"--quantify\", \"http://www.w3.org/2000/10/swap/var#\", \"--nope\"], norm_file, get_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize entire folder\n",
    "\n",
    "# path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/other_systems/gmark-dominik/50/\"\n",
    "path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix/\"\n",
    "for file in os.listdir(path):    \n",
    "    if not file.endswith(\".sparql\"):\n",
    "        continue\n",
    "    \n",
    "    file = file[:file.index(\".\")] + \".n3\"\n",
    "    or_file = os.path.join(path, \"n3\", file)\n",
    "    norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # ground\n",
    "    runNSave([\"eye\", \"--quiet\", or_file, \"--no-qvars\", \"--nope\", \"--pass-all\"], norm_file, get_times=False)\n",
    "    \n",
    "    # normalize\n",
    "    runNSave([\"eye\", \"--quiet\", norm_file, \"aux_list.n3\", \"--query\", \"list-predicate.n3\", \"--quantify\", \"http://www.w3.org/2000/10/swap/var#\", \"--nope\"], norm_file, get_times=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 145\n"
     ]
    }
   ],
   "source": [
    "# run single rule\n",
    "\n",
    "path = \"test\"\n",
    "norm_file = os.path.join(path, \"test2_norm.n3\")\n",
    "data_path = os.path.join(path, \"data.n3\")\n",
    "res_file = os.path.join(path, \"test2_rule_creation.n3\")\n",
    "\n",
    "# netw_time, reas_time = runNSave([\"eye\", data_path, \"property-paths-direct.n3\", \"--query\", norm_file, \"--nope\"], res_file)\n",
    "netw_time, reas_time = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation.n3\", \"--nope\"], res_file)\n",
    "print(netw_time, reas_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run folder\n",
    "\n",
    "# path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/other_systems/gmark-dominik/50/\"\n",
    "path = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix/\"\n",
    "\n",
    "times_file = open(os.path.join(path, \"n3\", \"results\", \"times.csv\"), 'w')\n",
    "times_file.write(\"query,data,type,phase,netw_time,reas_time\\n\")\n",
    "\n",
    "files = list(os.listdir(os.path.join(path, \"n3\", \"normalized\")))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.startswith(\"query\") and not file.endswith(\".n3\"):\n",
    "        continue\n",
    "    \n",
    "    name = file[:file.index(\".\")]\n",
    "    norm_file = os.path.join(path, \"n3\", \"normalized\", file)\n",
    "    data_file = \"data.n3\"\n",
    "    data_path = os.path.join(path, data_file)\n",
    "    dir_res_file = os.path.join(path, \"n3\", \"results\", \"direct\", file)\n",
    "    bwd_res_file = os.path.join(path, \"n3\", \"results\", \"bwd\", file)\n",
    "    fwd_res_file = os.path.join(path, \"n3\", \"results\", \"fwd\", file)\n",
    "    \n",
    "    print(norm_file)\n",
    "    \n",
    "    # - direct \n",
    "    netw_time, reas_time = runNSave([\"eye\", data_path, \"property-paths-direct.n3\", \"--query\", norm_file, \"--nope\"], dir_res_file)\n",
    "    record(times_file, file, data_file, 'direct', 'n/a', netw_time, reas_time)\n",
    "    \n",
    "    # - backward\n",
    "    tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_bwd.n3\")\n",
    "    netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation-backwards.n3\", \"--nope\"], tmp_file)\n",
    "    record(times_file, file, data_file, 'bwd', 'create', netw_time1, reas_time1)\n",
    "    netw_time2, reas_time2 = runNSave([\"eye\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], bwd_res_file)\n",
    "    record(times_file, file, data_file, 'bwd', 'run', netw_time2, reas_time2)\n",
    "    record(times_file, file, data_file, 'bwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "        \n",
    "    # - forward\n",
    "    tmp_file = os.path.join(path, \"n3\", \"gen\", f\"{name}_fwd.n3\")\n",
    "    netw_time1, reas_time1 = runNSave([\"eye\", norm_file, \"rule-creation.n3\", \"--query\", \"rule-creation.n3\", \"--nope\"], tmp_file)\n",
    "    record(times_file, file, data_file, 'fwd', 'create', netw_time1, reas_time1)\n",
    "    netw_time2, reas_time2 = runNSave([\"eye\", data_path, tmp_file, \"--query\", norm_file, \"--nope\"], fwd_res_file)\n",
    "    record(times_file, file, data_file, 'fwd', 'run', netw_time2, reas_time2)\n",
    "    record(times_file, file, data_file, 'fwd', 'total', netw_time1+netw_time2, reas_time1+reas_time2)\n",
    "    \n",
    "    times_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(file1, file2):\n",
    "    process = subprocess.Popen(['java', '-jar', \"compare_res.jar\", file1, file2], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, error = [ b.decode('UTF-8') for b in process.communicate() ]\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-0.n3\n",
      "results are the same!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path1 = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix/results\"\n",
    "path2 = \"/Users/wvw/git/n3/sparql2n3/SPARQL-to-N3/gmark_50_new/mix/n3/results/fwd\"\n",
    "files = list(os.listdir(path1))\n",
    "files.sort()\n",
    "for file in files:\n",
    "    if not file.endswith(\".n3\"):\n",
    "        continue\n",
    "    file1 = os.path.join(path1, file)\n",
    "    file2 = os.path.join(path2, file)\n",
    "    if not os.path.isfile(file2):\n",
    "        print(\"cannot find file2\")\n",
    "    else:\n",
    "        print(file)\n",
    "        compare_results(file1, file2)\n",
    "        print()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
